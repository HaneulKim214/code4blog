{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Table of contents:\n",
    "- What is ROC\n",
    "- What is AUC\n",
    "- What does it mean to be balanced? -> exactly 50/50 or what?\n",
    "- xaxis in ROC when dataset is:\n",
    "    - imbalanced : Precision as x-axis\n",
    "    - balanced: FPR as x-axis\n",
    "    - explain why certain x-axis is prefered"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Previously in <a link=\"https://medium.com/analytics-vidhya/classification-performance-metric-with-python-sklearn-d8342ac25898\">Classification Performance Metric with python Sklearn</a> we've covered various performance metrics in classification including ROC curve and AUC however they were briefly mentioned. \n",
    "\n",
    "Readers are assumed to have understanding about confusion matrix, precision, recall, TPR,  and FPR. If you don't, it is recommended to read previous blog.\n",
    "\n",
    "We will dive deeper into ROC to understand its pros/cons, AUC, and when it should be replaced with PR curve."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use same dataset as before, breast cancer dataset from sklearn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just for my own sake, I've labelled malignant as 1 and benign as 0 which is the opposite labelling from previous blog."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_curve, precision_recall_curve, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "bc = load_breast_cancer()\n",
    "df = pd.DataFrame(data=bc.data, columns=bc.feature_names)\n",
    "df[\"target\"] = bc.target\n",
    "df[\"target\"] = df[\"target\"].map({0:1, 1:0})\n",
    "\n",
    "print(\"memory size before \", df.memory_usage(deep=True).sum())\n",
    "df.iloc[:, :-1] = StandardScaler().fit_transform(df.iloc[:, :-1])\n",
    "df.iloc[:, :-1] = df.iloc[:, :-1].astype(np.float16)\n",
    "df.iloc[:, -1] = df.iloc[:, -1].astype(np.int8)\n",
    "print(\"memory size after \", df.memory_usage(deep=True).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfusionMatrix:\n",
    "    \"\"\"\n",
    "    Contains metrics used to create confusion matrix:\n",
    "    True positive, False positive, False negative, and True negative.\n",
    "    \n",
    "    This class implements various calculations such as fpr, tpr, recall, and precision.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    true_y : list\n",
    "             each element is true label\n",
    "    pred_y : list\n",
    "             each element is predicted label\n",
    "    \"\"\"\n",
    "    def __init__(self, true_y, pred_y):\n",
    "        self.tp = 0\n",
    "        self.fp = 0\n",
    "        self.fn = 0\n",
    "        self.tn = 0\n",
    "\n",
    "        for tr_y, pr_y in zip(true_y, pred_y):\n",
    "            # Positive\n",
    "            if pr_y == 1:\n",
    "                if tr_y == 1:\n",
    "                    self.tp += 1\n",
    "                elif tr_y == 0:\n",
    "                    self.fp += 1\n",
    "            # Negative\n",
    "            elif pr_y == 0:\n",
    "                if tr_y == 1:\n",
    "                    self.fn += 1\n",
    "                elif tr_y == 0:\n",
    "                    self.tn += 1\n",
    "                    \n",
    "    def calc_tpr(self):\n",
    "        \"\"\"Calculate tpr a.k.a. recall\"\"\"\n",
    "        try:\n",
    "            r = self.tp / (self.tp + self.fn)\n",
    "        except ZeroDivisionError:\n",
    "            r = 0\n",
    "        return r\n",
    "    \n",
    "    def calc_fpr(self):\n",
    "        \"\"\"Calculate fpr\"\"\"\n",
    "        return self.fp / (self.fp + self.tn) \n",
    "        \n",
    "    def calc_precision(self):\n",
    "        try:\n",
    "            pr = self.tp / (self.tp + self.fp)\n",
    "        except ZeroDivisionError:\n",
    "            pr = 0\n",
    "        return pr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing df with different fraction of class 1\n",
    "- df with different level of imbalanceness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class1_frac = [0.1, 0.3, 0.5, 0.7, 1]\n",
    "results_dfs = dict()\n",
    "\n",
    "for frac in class1_frac:\n",
    "    malignant_subset_df = df.loc[df[\"target\"]==1].sample(frac=frac)\n",
    "    new_df = pd.concat([malignant_subset_df, df.loc[df[\"target\"] == 0]])\n",
    "    \n",
    "    y = new_df[\"target\"].values\n",
    "    X = new_df.drop(columns=[\"target\"]).values\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.35)\n",
    "    log_reg = LogisticRegression().fit(X_train, y_train)\n",
    "    probs = log_reg.predict_proba(X_test)\n",
    "    \n",
    "    result_df = pd.DataFrame(probs, columns=[\"0\", \"1\"])\n",
    "    result_df[\"y_test\"] = y_test\n",
    "    \n",
    "    results_dfs[frac] = result_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each result_df, we will see how as __number of true negative become larger it lowers FPR__ making look as if performing good when it is not.\n",
    "To show why ROC is not good perf metrics when class is imbalanced. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df = results_dfs[0.1]\n",
    "y_pred = np.where(result_df[\"1\"] > 0.5, 1, 0)\n",
    "result_df[\"y_pred\"] = y_pred\n",
    "cm_10 = ConfusionMatrix(y_test, y_pred)\n",
    "\n",
    "class0_r, class1_r = round(result_df[\"y_test\"].value_counts(normalize=True),2).tolist()\n",
    "print(f\"class0:class_1 ratio = {class0_r}:{class1_r}\" )\n",
    "print(\"% of correctly classifying class 0 = \", round(cm_10.tn / (cm_10.tn + cm_10.fp), 3))\n",
    "print(\"% of correctly classifying class 1 = \", round(cm_10.tp / (cm_10.tp + cm_10.fn), 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df = results_dfs[0.3]\n",
    "y_pred = np.where(result_df[\"1\"] > 0.5, 1, 0)\n",
    "result_df[\"y_pred\"] = y_pred\n",
    "cm_30 = ConfusionMatrix(y_test, y_pred)\n",
    "\n",
    "class0_r, class1_r = round(result_df[\"y_test\"].value_counts(normalize=True),2).tolist()\n",
    "print(f\"class0:class_1 ratio = {class0_r}:{class1_r}\" )\n",
    "print(\"% of correctly classifying class 0 = \", round(cm_30.tn / (cm_30.tn + cm_30.fp), 3))\n",
    "print(\"% of correctly classifying class 1 = \", round(cm_30.tp / (cm_30.tp + cm_30.fn), 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df = results_dfs[0.7]\n",
    "y_pred = np.where(result_df[\"1\"] > 0.5, 1, 0)\n",
    "result_df[\"y_pred\"] = y_pred\n",
    "cm_70 = ConfusionMatrix(y_test, y_pred)\n",
    "\n",
    "class0_r, class1_r = round(result_df[\"y_test\"].value_counts(normalize=True),2).tolist()\n",
    "print(f\"class0:class_1 ratio = {class0_r}:{class1_r}\" )\n",
    "print(\"% of correctly classifying class 0 = \", round(cm_70.tn / (cm_70.tn + cm_70.fp), 3))\n",
    "print(\"% of correctly classifying class 1 = \", round(cm_70.tp / (cm_70.tp + cm_70.fn), 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC using custom tpr, fpr calculator\n",
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        name = \"random guessing\",\n",
    "        x = np.linspace(0, 1, 6),\n",
    "        y = np.linspace(0, 1, 6),\n",
    "        line = dict(dash='dash')\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"ROC\",\n",
    "    xaxis_title=\"False Positive Rate\",\n",
    "    yaxis_title=\"True Positive Rate\"\n",
    ")\n",
    "\n",
    "def add_to_roc_plot(fig, frac, thresholds, tpr, fpr):\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            name = f\"frac={frac} - Logistic regression\",\n",
    "            x=fpr,\n",
    "            y=tpr,\n",
    "            mode='lines+markers+text',\n",
    "#             text=np.round(thresholds, 2),\n",
    "#             textposition='top right',\n",
    "#             textfont = dict(\n",
    "#                 family=\"sans serif\",\n",
    "#                 size=18,\n",
    "#                 color=\"LightSeaGreen\"\n",
    "#             )\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholds = np.linspace(0, 1, 10)\n",
    "\n",
    "for frac, rdf in results_dfs.items():\n",
    "    tpr = list()\n",
    "    fpr = list()\n",
    "    precision_list = list()\n",
    "    y_test = rdf[\"y_test\"]\n",
    "    for thld in thresholds:\n",
    "        y_pred = np.where(rdf[\"1\"] > thld, 1, 0)\n",
    "        cm = ConfusionMatrix(y_test, y_pred)\n",
    "\n",
    "        tpr.append(cm.calc_tpr())\n",
    "        fpr.append(cm.calc_fpr())\n",
    "        precision_list.append(cm.calc_precision())\n",
    "        \n",
    "    add_to_roc_plot(fig, frac, thresholds, tpr, fpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC using custom tpr, fpr calculator\n",
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        name = \"Logistic regression\",\n",
    "        x=fpr,\n",
    "        y=tpr,\n",
    "        mode='lines+markers+text',\n",
    "        text=np.round(thresholds, 2),\n",
    "        textposition='top right',\n",
    "        textfont = dict(\n",
    "            family=\"sans serif\",\n",
    "            size=18,\n",
    "            color=\"LightSeaGreen\"\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        name = \"random guessing\",\n",
    "        x = np.linspace(0, 1, 6),\n",
    "        y = np.linspace(0, 1, 6),\n",
    "        line = dict(dash='dash')\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"ROC\",\n",
    "    xaxis_title=\"False Positive Rate\",\n",
    "    yaxis_title=\"True Positive Rate\"\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom PR Curve\n",
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        name = \"Logistic regression\",\n",
    "        x=tpr,\n",
    "        y=precision_list,\n",
    "        mode='lines+markers+text',\n",
    "        text=np.round(thresholds, 2),\n",
    "        textposition='top right',\n",
    "        textfont = dict(\n",
    "            family=\"sans serif\",\n",
    "            size=18,\n",
    "            color=\"LightSeaGreen\"\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        name = \"random guessing\",\n",
    "        x = np.linspace(0, 1, 6),\n",
    "        y = np.linspace(0, 1, 6),\n",
    "        line = dict(dash='dash')\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"Precision-Recall Curve\",\n",
    "    xaxis_title=\"Recall\",\n",
    "    yaxis_title=\"Precision\"\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
