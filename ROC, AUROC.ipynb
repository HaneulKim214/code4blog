{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Table of contents:\n",
    "- What is ROC\n",
    "- What is AUC\n",
    "- What does it mean to be balanced? -> exactly 50/50 or what?\n",
    "- xaxis in ROC when dataset is:\n",
    "    - imbalanced : Precision as x-axis\n",
    "    - balanced: FPR as x-axis\n",
    "    - explain why certain x-axis is prefered"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Previously in <a link=\"https://medium.com/analytics-vidhya/classification-performance-metric-with-python-sklearn-d8342ac25898\">Classification Performance Metric with python Sklearn</a> we've covered various performance metrics in classification including ROC curve and AUC however they were briefly mentioned. \n",
    "\n",
    "Readers are assumed to have understanding about confusion matrix, precision, recall, TPR,  and FPR. If you don't, it is recommended to read previous blog.\n",
    "\n",
    "We will dive deeper into ROC to understand its pros/cons, AUC, and when it should be replaced with PR curve."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use same dataset as before, breast cancer dataset from sklearn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just for my own sake, I've labelled malignant as 1 and benign as 0 which is the opposite labelling from previous blog."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "memory size before  141240\n",
      "memory size after  34837\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_curve, precision_recall_curve, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "bc = load_breast_cancer()\n",
    "df = pd.DataFrame(data=bc.data, columns=bc.feature_names)\n",
    "df[\"target\"] = bc.target\n",
    "df[\"target\"] = df[\"target\"].map({0:1, 1:0})\n",
    "\n",
    "print(\"memory size before \", df.memory_usage(deep=True).sum())\n",
    "df.iloc[:, :-1] = StandardScaler().fit_transform(df.iloc[:, :-1])\n",
    "df.iloc[:, :-1] = df.iloc[:, :-1].astype(np.float16)\n",
    "df.iloc[:, -1] = df.iloc[:, -1].astype(np.int8)\n",
    "print(\"memory size after \", df.memory_usage(deep=True).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = new_df[\"target\"].values\n",
    "X = new_df.drop(columns=[\"target\"]).values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class1_frac = [0.1, 0.3, 0.5, 0.7, 1]\n",
    "\n",
    "# df with different level of imbalanceness\n",
    "dfs = dict()\n",
    "\n",
    "for frac in class1_frac:\n",
    "    malignant_subset_df = df.loc[df[\"target\"]==1].sample(frac=frac)\n",
    "    new_df = pd.concat([malignant_subset_df, df.loc[df[\"target\"] == 0]])\n",
    "    dfs[frac] = new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1\n",
      "class0:class_1 ratio = 0.94:0.06\n",
      "----------\n",
      "0.3\n",
      "class0:class_1 ratio = 0.85:0.15\n",
      "----------\n",
      "0.5\n",
      "class0:class_1 ratio = 0.77:0.23\n",
      "----------\n",
      "0.7\n",
      "class0:class_1 ratio = 0.71:0.29\n",
      "----------\n",
      "1\n",
      "class0:class_1 ratio = 0.63:0.37\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "for frac, df in dfs.items():\n",
    "    class0_r, class1_r = round(df[\"target\"].value_counts(normalize=True),2).tolist()\n",
    "    print(frac)\n",
    "    print(f\"class0:class_1 ratio = {class0_r}:{class1_r}\" )\n",
    "    print(\"----------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg = LogisticRegression()\n",
    "log_reg.fit(X_train, y_train)\n",
    "\n",
    "y_pred = log_reg.predict(X_test)\n",
    "probas = log_reg.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df = pd.DataFrame(probas, columns=[\"0\", \"1\"])\n",
    "result_df[\"true_y\"] = y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to plot ROC we need to measure FPR, TPR at different thresholds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfusionMatrix:\n",
    "    \"\"\"\n",
    "    Contains metrics used to create confusion matrix:\n",
    "    True positive, False positive, False negative, and True negative.\n",
    "    \n",
    "    This class implements various calculations such as fpr, tpr, recall, and precision.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    true_y : list\n",
    "             each element is true label\n",
    "    pred_y : list\n",
    "             each element is predicted label\n",
    "    \"\"\"\n",
    "    def __init__(self, true_y, pred_y):\n",
    "        self.tp = 0\n",
    "        self.fp = 0\n",
    "        self.fn = 0\n",
    "        self.tn = 0\n",
    "\n",
    "        for tr_y, pr_y in zip(true_y, pred_y):\n",
    "            # Positive\n",
    "            if pr_y == 1:\n",
    "                if tr_y == 1:\n",
    "                    self.tp += 1\n",
    "                elif tr_y == 0:\n",
    "                    self.fp += 1\n",
    "            # Negative\n",
    "            elif pr_y == 0:\n",
    "                if tr_y == 1:\n",
    "                    self.fn += 1\n",
    "                elif tr_y == 0:\n",
    "                    self.tn += 1\n",
    "                    \n",
    "    def calc_tpr(self):\n",
    "        \"\"\"Calculate tpr a.k.a. recall\"\"\"\n",
    "        try:\n",
    "            r = self.tp / (self.tp + self.fn)\n",
    "        except ZeroDivisionError:\n",
    "            r = 0\n",
    "        return r\n",
    "    \n",
    "    def calc_fpr(self):\n",
    "        \"\"\"Calculate fpr\"\"\"\n",
    "        return self.fp / (self.fp + self.tn) \n",
    "        \n",
    "    def calc_precision(self):\n",
    "        try:\n",
    "            pr = self.tp / (self.tp + self.fp)\n",
    "        except ZeroDivisionError:\n",
    "            pr = 0\n",
    "        return pr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tpr = list()\n",
    "fpr = list()\n",
    "precision_list = list()\n",
    "\n",
    "thresholds = np.linspace(0, 1, 10)\n",
    "for thld in thresholds:\n",
    "    pred_y = np.where(result_df[\"1\"] > thld, 1, 0)\n",
    "    cm = ConfusionMatrix(y_test, pred_y)\n",
    "    \n",
    "    tpr.append(cm.calc_tpr())\n",
    "    fpr.append(cm.calc_fpr())\n",
    "    precision_list.append(cm.calc_precision())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC using custom tpr, fpr calculator\n",
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        name = \"Logistic regression\",\n",
    "        x=fpr,\n",
    "        y=tpr,\n",
    "        mode='lines+markers+text',\n",
    "        text=np.round(thresholds, 2),\n",
    "        textposition='top right',\n",
    "        textfont = dict(\n",
    "            family=\"sans serif\",\n",
    "            size=18,\n",
    "            color=\"LightSeaGreen\"\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        name = \"random guessing\",\n",
    "        x = np.linspace(0, 1, 6),\n",
    "        y = np.linspace(0, 1, 6),\n",
    "        line = dict(dash='dash')\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"ROC\",\n",
    "    xaxis_title=\"False Positive Rate\",\n",
    "    yaxis_title=\"True Positive Rate\"\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check ROC with as class goes from imbalanced to balanced with same % of wrongly predicting positives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom PR Curve\n",
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        name = \"Logistic regression\",\n",
    "        x=tpr,\n",
    "        y=precision_list,\n",
    "        mode='lines+markers+text',\n",
    "        text=np.round(thresholds, 2),\n",
    "        textposition='top right',\n",
    "        textfont = dict(\n",
    "            family=\"sans serif\",\n",
    "            size=18,\n",
    "            color=\"LightSeaGreen\"\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        name = \"random guessing\",\n",
    "        x = np.linspace(0, 1, 6),\n",
    "        y = np.linspace(0, 1, 6),\n",
    "        line = dict(dash='dash')\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"Precision-Recall Curve\",\n",
    "    xaxis_title=\"Recall\",\n",
    "    yaxis_title=\"Precision\"\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC using sklearn library\n",
    "scores = probas[:, 1]\n",
    "fpr, tpr, thresholds = roc_curve(y_test, scores)\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        name = \"Logistic regression\",\n",
    "        x=fpr,\n",
    "        y=tpr,\n",
    "        text=thresholds\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        name = \"random guessing\",\n",
    "        x = np.linspace(0, 1, 6),\n",
    "        y = np.linspace(0, 1, 6),\n",
    "        line = dict(dash='dash')\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"ROC\",\n",
    "    xaxis_title=\"False Positive Rate\",\n",
    "    yaxis_title=\"True Positive Rate\"\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = 0\n",
    "for row in result_df.itertuples():\n",
    "    true_y = getattr(row, \"target\")\n",
    "    pred_y = getattr(row, \"pred_t\")\n",
    "    \n",
    "    if true_y == pred_y:\n",
    "        correct += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct / len(result_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df.loc[result_df[\"target\"] != result_df[\"pred_t\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.where(result_df[\"0\"] > 0.5, 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df.loc[(result_df[\"0\"] >= 0.4) &\n",
    "              (result_df[\"0\"] <= 0.6)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1/(1+np.exp(z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x = result_df[\"1\"],\n",
    "        y = result_df[\"1\"],\n",
    "        mode=\"markers\"\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is ROC\n",
    "\n",
    "- show how ROC is created under the hood of sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "balanced => exactly 50/50?\n",
    "\n",
    "## ROC Vs. PR curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>References</b>\n",
    "\n",
    "- https://developers.google.com/machine-learning/crash-course/classification/roc-and-auc"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
